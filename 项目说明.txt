一、整个项目流程：
   1、通过预处理实验材料（复旦大学微博数据），赛选出潜在垃圾用户
   2、通过爬虫去现在的新浪微博，爬这些用户，提取那些现在不存在了的用户。
   3、现在不存在的这部分用户（可能是一些原因被封），提取这些用户的微博信息，分析特征，分类等采用数据挖掘的方法进行分析
   4、再检验调整，以期望识别微博水军及垃圾用户，具有比较好的效果。
   
   
   
   
   
   
   *********************************
            进度说明
   *********************************
二、
1、根据标识过滤现有的微博用户，目的是过滤掉那些认证用户（认证用户是垃圾用户的可能性不大）
2、提取用户名，进行预处理，目的是过滤掉大多正常用户。方法是，对用户名进行Ngram相似的分析，社会学上来讲，一般来讲人的用户名大多具有一些意义，
       所以一些词可能是大家命名时都爱使用的，这些词组成的名字相对来讲是水军垃圾用户的可能性要比随机一些用户来讲低一些。所以这一步的目的就是，进行Ngram
       分析按出现概率降序排列用户名，然后取部分作为潜在垃圾用户，以对他们进行进一步的分析。（实验过程中，发现一些两个字的及三个字的用户名作为低频大量出现，
       原因是这些大多是真的人名，作为人名一般不是词语或者常见的，所以在整个用户名组成的空间中出现的概率就比较低，而对于我们的目的来讲，显然这部分人是水军 的
       可能性也比较低，所以对这部分还需要舍弃。现在的做法是，去除四个字一下的用户名，然后去低频的25万人的用户作为潜在垃圾用户列表保存）
3、从源文件提取关注关系，第一步得到的25W个用户uid,去源文件follows_users.csv提取他们的关注关系，包括他们关注别人的，以及别粉丝关注，两方面的
       关系，分别都表示出来。
       由于爬取的原因，实验发现在25W个用户在源关注关系中只有七万多个有关注别人，也只有七万多人有被别人关注。由于要进行再爬取，所以这里挑选那些既有关注别人
       也有别人关注的用户uid提取出来，保存为潜在垃圾用户列表。并且提取关系，把用户表示为他关注的用户的uid，以及把用户表示为他粉丝uid。
4、把用户表示为他关注的用户的uid的词向量，这里使用的是word2vec深度学习的词向量工具。（这一步可能后期才能用到，现在只是了解word2vec的直观使用）